{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A notebook for manually implementing classic machine learning and deep learning algorithms (without using ready-made high-level functions from Sklearn/PyTorch).\n",
        "\n",
        "# This notebook includes the manual implementation of some newer models: Like BERT, AlexNet, ResNet.\n",
        "\n",
        "# This notebook also includes the manual implementation of classic models: Like RNN, LSTM, CNN, MLP, logistic regression, ridge regression, decision tree, random forest, and XGBoost.\n",
        "\n",
        "# I'm still keeping updated"
      ],
      "metadata": {
        "id": "sD4_RDxSgy2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT"
      ],
      "metadata": {
        "id": "WrWTnxyXrgSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_size // num_heads\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * num_heads == embed_size\n",
        "        ), \"Embedding size needs to be divisible by num_heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, embed_size, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, embed_size, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, embed_size, bias=False)\n",
        "        self.fc_out = nn.Linear(num_heads * self.head_dim, embed_size)\n",
        "\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        N = query.shape[0]\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "        # Split the embedding into self.num_heads different pieces\n",
        "        values = values.reshape(N, value_len, self.num_heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.num_heads, self.head_dim)\n",
        "        queries = query.reshape(N, query_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
        "\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "            N, query_len, self.num_heads * self.head_dim\n",
        "        )\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_size, forward_expansion):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(embed_size, forward_expansion * embed_size)\n",
        "        self.fc2 = nn.Linear(forward_expansion * embed_size, embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(F.gelu(self.fc1(x)))\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads, forward_expansion, dropout):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadSelfAttention(embed_size, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.feed_forward = FeedForward(embed_size, forward_expansion)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "        x = self.dropout(self.norm1(attention + query))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out\n",
        "\n",
        "class BERT(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        num_heads,\n",
        "        forward_expansion,\n",
        "        vocab_size,\n",
        "        max_length,\n",
        "        dropout,\n",
        "    ):\n",
        "        super(BERT, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerBlock(\n",
        "                    embed_size, num_heads, forward_expansion, dropout\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        N, seq_length = x.shape\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(x.device)\n",
        "        out = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
        "\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, out, out, mask)\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vocab_size = 30522  # Vocabulary size for BERT (usually 30k+)\n",
        "embed_size = 256  # Embedding size\n",
        "num_layers = 6  # Number of transformer blocks\n",
        "num_heads = 8  # Number of heads in multi-head attention\n",
        "forward_expansion = 4  # Expansion factor in feed forward network\n",
        "max_length = 100  # Maximum length of input sequence\n",
        "dropout = 0.1  # Dropout rate\n",
        "\n",
        "# Initialize model\n",
        "model = BERT(\n",
        "    embed_size,\n",
        "    num_layers,\n",
        "    num_heads,\n",
        "    forward_expansion,\n",
        "    vocab_size,\n",
        "    max_length,\n",
        "    dropout,\n",
        ").to(device)\n",
        "\n",
        "# Example input\n",
        "input_ids = torch.randint(0, vocab_size, (1, max_length)).to(device)  # Random input for demonstration\n",
        "mask = None  # For simplicity, no mask is applied here\n",
        "\n",
        "# Forward pass\n",
        "output = model(input_ids, mask)\n",
        "print(output.shape)  # Should output (batch_size, seq_length, vocab_size)\n",
        "\n",
        "# Loss and optimizer (for training purposes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
        "\n",
        "# Dummy training loop for demonstration purposes\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(input_ids, mask)\n",
        "    # Shift prediction to align with target labels\n",
        "    loss = criterion(outputs.view(-1, vocab_size), input_ids.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oiVh-V7ra7f",
        "outputId": "01ebe9a3-f647-4aa8-9356-5d0a23468baa"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 100, 30522])\n",
            "Epoch 1, Loss: 10.583015441894531\n",
            "Epoch 2, Loss: 10.463675498962402\n",
            "Epoch 3, Loss: 10.540785789489746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AlexNet"
      ],
      "metadata": {
        "id": "E_iSSvx7rpbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define AlexNet model class\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Setting device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Transformations for the training and testing sets\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((227, 227)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# Define the model, loss function, and optimizer\n",
        "num_classes = 10\n",
        "model = AlexNet(num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(trainloader, 0):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:    # Print every 100 mini-batches\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(trainloader)}], Loss: {running_loss / 100:.4f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Testing the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')\n",
        "\n",
        "# Testing the model on each class\n",
        "classes = trainset.classes\n",
        "class_correct = list(0. for i in range(num_classes))\n",
        "class_total = list(0. for i in range(num_classes))\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(len(labels)):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "for i in range(num_classes):\n",
        "    print(f'Accuracy of {classes[i]} : {100 * class_correct[i] / class_total[i]:.2f}%')\n",
        "\n",
        "print('Testing Completed')"
      ],
      "metadata": {
        "id": "-znLYoQ5s8n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet"
      ],
      "metadata": {
        "id": "mqqh5U0xrHLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "# 1. Data Loading and Preprocessing for Custom Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Assuming your dataset is in the 'data/' directory\n",
        "train_dataset = ImageFolder(root='./data/train', transform=transform)\n",
        "test_dataset = ImageFolder(root='./data/test', transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# 2. Defining the Residual Block\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.downsample = None\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# 3. Defining the ResNet Model\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=2):  # Changed num_classes to 2 (cat and dog)\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# 4. Creating the ResNet-18 Model\n",
        "# ResNet-18 has [2, 2, 2, 2] blocks in each of the 4 layers\n",
        "model = ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# 5. Loss Function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 6. Training the Model\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 10 == 9:  # print every 10 mini-batches\n",
        "                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 10:.4f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, criterion, optimizer, num_epochs=10)\n",
        "\n",
        "# 7. Evaluating the Model\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(model, test_loader)\n",
        "\n",
        "# 8. Saving the Model\n",
        "torch.save(model.state_dict(), 'resnet18_cats_dogs.pth')"
      ],
      "metadata": {
        "id": "B_SJAzrFrHdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent neural network, RNN"
      ],
      "metadata": {
        "id": "htVesuS_m7HZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Manually implemented RNN\n",
        "class RNNManual:\n",
        "    def __init__(self, input_size, hidden_size, output_size, seq_length):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        # Initialize weights\n",
        "        self.Wxh = torch.randn(hidden_size, input_size, requires_grad=True) * 0.1\n",
        "        self.Whh = torch.randn(hidden_size, hidden_size, requires_grad=True) * 0.1\n",
        "        self.Why = torch.randn(output_size, hidden_size, requires_grad=True) * 0.1\n",
        "        self.bh = torch.zeros(hidden_size, requires_grad=True)\n",
        "        self.by = torch.zeros(output_size, requires_grad=True)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        h = torch.zeros(self.hidden_size, requires_grad=False)  # Initialize hidden state\n",
        "        self.last_hs = []  # Store hidden state at each time step\n",
        "\n",
        "        # Loop through time steps for forward propagation\n",
        "        for t in range(self.seq_length):\n",
        "            x = inputs[t]\n",
        "            h = torch.tanh(self.Wxh @ x + self.Whh @ h + self.bh)  # Calculate new hidden state\n",
        "            self.last_hs.append(h)  # Save hidden state for backpropagation\n",
        "\n",
        "        y = self.Why @ h + self.by  # Final output\n",
        "        return y, h\n",
        "\n",
        "    def backward(self, inputs, dL_dy, learning_rate=0.001):\n",
        "        dWhy = torch.zeros_like(self.Why)\n",
        "        dby = torch.zeros_like(self.by)\n",
        "        dWxh = torch.zeros_like(self.Wxh)\n",
        "        dWhh = torch.zeros_like(self.Whh)\n",
        "        dbh = torch.zeros_like(self.bh)\n",
        "\n",
        "        # Gradients for output layer\n",
        "        dWhy += dL_dy.view(-1, 1) @ self.last_hs[-1].view(1, -1)\n",
        "        dby += dL_dy\n",
        "\n",
        "        # Backpropagation through time (BPTT)\n",
        "        dL_dh_next = torch.zeros(self.hidden_size)\n",
        "        for t in reversed(range(self.seq_length)):\n",
        "            dL_dh = dL_dh_next + (self.Why.T @ dL_dy if t == self.seq_length - 1 else 0)\n",
        "            dh_raw = (1 - self.last_hs[t] ** 2) * dL_dh  # Derivative of tanh\n",
        "\n",
        "            dbh += dh_raw\n",
        "            dWxh += dh_raw.view(-1, 1) @ inputs[t].view(1, -1)\n",
        "            if t > 0:\n",
        "                dWhh += dh_raw.view(-1, 1) @ self.last_hs[t-1].view(1, -1)\n",
        "\n",
        "            dL_dh_next = self.Whh.T @ dh_raw\n",
        "\n",
        "        # Gradient descent to update parameters\n",
        "        self.Wxh.data -= learning_rate * dWxh\n",
        "        self.Whh.data -= learning_rate * dWhh\n",
        "        self.Why.data -= learning_rate * dWhy\n",
        "        self.bh.data -= learning_rate * dbh\n",
        "        self.by.data -= learning_rate * dby\n",
        "\n",
        "# Data preparation\n",
        "def toy_data():\n",
        "    # A simple sequence mapping problem, e.g., input [1, 2, 3] -> output [0.1]\n",
        "    inputs = [torch.tensor([i], dtype=torch.float32) for i in range(1, 4)]\n",
        "    target = torch.tensor([0.1], dtype=torch.float32)\n",
        "    return inputs, target\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = 1\n",
        "hidden_size = 10\n",
        "output_size = 1\n",
        "seq_length = 3\n",
        "learning_rate = 0.01\n",
        "epochs = 500\n",
        "\n",
        "# Instantiate the model\n",
        "rnn = RNNManual(input_size, hidden_size, output_size, seq_length)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    inputs, target = toy_data()\n",
        "\n",
        "    # Forward pass\n",
        "    output, _ = rnn.forward(inputs)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = (output - target).pow(2).sum() * 0.5\n",
        "\n",
        "    # Manually clear gradients\n",
        "    rnn.Wxh.grad = None\n",
        "    rnn.Whh.grad = None\n",
        "    rnn.Why.grad = None\n",
        "    rnn.bh.grad = None\n",
        "    rnn.by.grad = None\n",
        "\n",
        "    # Backward pass\n",
        "    dL_dy = output - target\n",
        "    rnn.backward(inputs, dL_dy, learning_rate)\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "# Testing\n",
        "test_inputs, _ = toy_data()\n",
        "test_output, _ = rnn.forward(test_inputs)\n",
        "print('Test Output:', test_output.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCLnhSxHm8NB",
        "outputId": "1e0ab3ce-2e24-4f2b-a9c8-d63150a3d82f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.008545284159481525\n",
            "Epoch 50, Loss: 5.8329940657131374e-05\n",
            "Epoch 100, Loss: 3.8827207049507706e-07\n",
            "Epoch 150, Loss: 2.5777093792811456e-09\n",
            "Epoch 200, Loss: 1.7322254741714005e-11\n",
            "Epoch 250, Loss: 1.6456280782506383e-13\n",
            "Epoch 300, Loss: 1.5987211554602254e-14\n",
            "Epoch 350, Loss: 3.9968028886505635e-15\n",
            "Epoch 400, Loss: 3.9968028886505635e-15\n",
            "Epoch 450, Loss: 3.9968028886505635e-15\n",
            "Test Output: 0.09999992698431015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Long short-term memory, LSTM"
      ],
      "metadata": {
        "id": "ZOPRIpQomjTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# 1. Define the manually implemented LSTM class\n",
        "class ManualLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(ManualLSTM, self).__init__()\n",
        "\n",
        "        # LSTM size parameters\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Input gate parameters\n",
        "        self.W_i = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.U_i = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_i = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # Forget gate parameters\n",
        "        self.W_f = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.U_f = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_f = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # Output gate parameters\n",
        "        self.W_o = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.U_o = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_o = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # Cell state parameters\n",
        "        self.W_c = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.U_c = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_c = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # Output layer parameters\n",
        "        self.W_y = nn.Parameter(torch.Tensor(hidden_size, output_size))\n",
        "        self.b_y = nn.Parameter(torch.Tensor(output_size))\n",
        "\n",
        "        # Parameter initialization\n",
        "        self.init_parameters()\n",
        "\n",
        "    def init_parameters(self):\n",
        "        for param in self.parameters():\n",
        "            nn.init.uniform_(param, -0.08, 0.08)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        h_prev, c_prev = hidden\n",
        "        outputs = []\n",
        "\n",
        "        # For each time step in the sequence\n",
        "        for t in range(x.size(1)):\n",
        "            x_t = x[:, t, :]\n",
        "\n",
        "            # Input gate calculation\n",
        "            i_t = torch.sigmoid(x_t @ self.W_i + h_prev @ self.U_i + self.b_i)\n",
        "\n",
        "            # Forget gate calculation\n",
        "            f_t = torch.sigmoid(x_t @ self.W_f + h_prev @ self.U_f + self.b_f)\n",
        "\n",
        "            # Output gate calculation\n",
        "            o_t = torch.sigmoid(x_t @ self.W_o + h_prev @ self.U_o + self.b_o)\n",
        "\n",
        "            # Candidate cell state calculation\n",
        "            c_hat_t = torch.tanh(x_t @ self.W_c + h_prev @ self.U_c + self.b_c)\n",
        "\n",
        "            # Current cell state\n",
        "            c_t = f_t * c_prev + i_t * c_hat_t\n",
        "\n",
        "            # Current hidden state\n",
        "            h_t = o_t * torch.tanh(c_t)\n",
        "\n",
        "            # Save current output\n",
        "            outputs.append(h_t.unsqueeze(1))\n",
        "\n",
        "            # Update the previous hidden state and cell state\n",
        "            h_prev, c_prev = h_t, c_t\n",
        "\n",
        "        # Concatenate outputs from all time steps\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "\n",
        "        # Use the output of the last time step for prediction\n",
        "        y = outputs[:, -1, :] @ self.W_y + self.b_y\n",
        "        return y, (h_t, c_t)\n",
        "\n",
        "# 2. Data generation and model training\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seed\n",
        "    torch.manual_seed(0)\n",
        "    np.random.seed(0)\n",
        "\n",
        "    # Create a simple dataset, for example, fitting a sine function\n",
        "    time_steps = np.linspace(0, np.pi * 2, 100)\n",
        "    data = np.sin(time_steps)\n",
        "    X = []\n",
        "    y = []\n",
        "    seq_length = 10\n",
        "\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    X = torch.Tensor(X).unsqueeze(-1)  # shape: (batch, seq_length, input_size)\n",
        "    y = torch.Tensor(y).unsqueeze(-1)  # shape: (batch, output_size)\n",
        "\n",
        "    # Model parameters\n",
        "    input_size = 1\n",
        "    hidden_size = 32\n",
        "    output_size = 1\n",
        "\n",
        "    # Instantiate the model\n",
        "    model = ManualLSTM(input_size, hidden_size, output_size)\n",
        "\n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    # Train the model\n",
        "    n_epochs = 200\n",
        "    for epoch in range(n_epochs):\n",
        "        # Initialize hidden state and cell state\n",
        "        h_0 = torch.zeros(X.size(0), hidden_size)\n",
        "        c_0 = torch.zeros(X.size(0), hidden_size)\n",
        "\n",
        "        # Forward pass\n",
        "        output, (h_n, c_n) = model(X, (h_0, c_0))\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(output, y)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print the loss\n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Test the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_input = torch.Tensor(data[-seq_length:]).unsqueeze(0).unsqueeze(-1)\n",
        "        h_0 = torch.zeros(1, hidden_size)\n",
        "        c_0 = torch.zeros(1, hidden_size)\n",
        "        test_output, _ = model(test_input, (h_0, c_0))\n",
        "        print(f'Test Output: {test_output.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUxYHawxmkxT",
        "outputId": "c8d8dca0-1b6f-4448-dbc0-1dc4d78a65d8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/200], Loss: 0.0644\n",
            "Epoch [40/200], Loss: 0.0278\n",
            "Epoch [60/200], Loss: 0.0143\n",
            "Epoch [80/200], Loss: 0.0052\n",
            "Epoch [100/200], Loss: 0.0005\n",
            "Epoch [120/200], Loss: 0.0001\n",
            "Epoch [140/200], Loss: 0.0000\n",
            "Epoch [160/200], Loss: 0.0000\n",
            "Epoch [180/200], Loss: 0.0000\n",
            "Epoch [200/200], Loss: 0.0000\n",
            "Test Output: 0.0623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional neural network, CNN"
      ],
      "metadata": {
        "id": "M3NLqFEVmglX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Data preparation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Manually implement convolutional layer\n",
        "class ManualConv2d:\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        # Initialize convolutional kernel and bias\n",
        "        self.weight = torch.randn(out_channels, in_channels, kernel_size, kernel_size) * 0.1\n",
        "        self.bias = torch.randn(out_channels) * 0.1\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get input and output dimensions\n",
        "        batch_size, in_channels, height, width = x.shape\n",
        "        out_height = (height - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
        "        out_width = (width - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
        "        # Initialize output\n",
        "        out = torch.zeros((batch_size, self.out_channels, out_height, out_width))\n",
        "        # Add padding to input\n",
        "        if self.padding > 0:\n",
        "            x = F.pad(x, (self.padding, self.padding, self.padding, self.padding))\n",
        "        # Manually implement convolution operation\n",
        "        for b in range(batch_size):\n",
        "            for o in range(self.out_channels):\n",
        "                for i in range(in_channels):\n",
        "                    for h in range(0, out_height):\n",
        "                        for w in range(0, out_width):\n",
        "                            h_start = h * self.stride\n",
        "                            w_start = w * self.stride\n",
        "                            h_end = h_start + self.kernel_size\n",
        "                            w_end = w_start + self.kernel_size\n",
        "                            out[b, o, h, w] += torch.sum(x[b, i, h_start:h_end, w_start:w_end] * self.weight[o, i])\n",
        "                out[b, o] += self.bias[o]\n",
        "        return out\n",
        "\n",
        "# Manually implement max pooling layer\n",
        "class ManualMaxPool2d:\n",
        "    def __init__(self, kernel_size, stride):\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, height, width = x.shape\n",
        "        out_height = (height - self.kernel_size) // self.stride + 1\n",
        "        out_width = (width - self.kernel_size) // self.stride + 1\n",
        "        out = torch.zeros((batch_size, channels, out_height, out_width))\n",
        "        for b in range(batch_size):\n",
        "            for c in range(channels):\n",
        "                for h in range(out_height):\n",
        "                    for w in range(out_width):\n",
        "                        h_start = h * self.stride\n",
        "                        w_start = w * self.stride\n",
        "                        h_end = h_start + self.kernel_size\n",
        "                        w_end = w_start + self.kernel_size\n",
        "                        out[b, c, h, w] = torch.max(x[b, c, h_start:h_end, w_start:w_end])\n",
        "        return out\n",
        "\n",
        "# Build manual CNN model\n",
        "class ManualCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ManualCNN, self).__init__()\n",
        "        self.conv1 = ManualConv2d(in_channels=1, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = ManualMaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(8 * 14 * 14, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 8 * 14 * 14)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Training and testing functions\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}]\tLoss: {loss.item():.6f}')\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n",
        "          f' ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
        "\n",
        "# Device configuration and model initialization\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ManualCNN().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ],
      "metadata": {
        "id": "DX5v_ZwKkYQ3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Layer Perceptron, MLP"
      ],
      "metadata": {
        "id": "k7U6HzrBoGpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create synthetic data\n",
        "np.random.seed(42)\n",
        "x_train = np.random.rand(100, 2)\n",
        "y_train = (x_train[:, 0] + x_train[:, 1] > 1).astype(np.float32).reshape(-1, 1)\n",
        "\n",
        "x_train = torch.tensor(x_train, dtype=torch.float32).to(device)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
        "\n",
        "# Define the architecture of the multilayer perceptron\n",
        "input_size = 2\n",
        "hidden_size = 5\n",
        "output_size = 1\n",
        "\n",
        "# Randomly initialize weights and biases\n",
        "w1 = torch.randn(input_size, hidden_size, device=device, requires_grad=True)\n",
        "b1 = torch.randn(hidden_size, device=device, requires_grad=True)\n",
        "w2 = torch.randn(hidden_size, output_size, device=device, requires_grad=True)\n",
        "b2 = torch.randn(output_size, device=device, requires_grad=True)\n",
        "\n",
        "# Define hyperparameters\n",
        "learning_rate = 0.01\n",
        "num_epochs = 1000\n",
        "\n",
        "# Start training\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward propagation\n",
        "    hidden_layer = torch.matmul(x_train, w1) + b1\n",
        "    hidden_layer_activation = torch.sigmoid(hidden_layer)\n",
        "    output_layer = torch.matmul(hidden_layer_activation, w2) + b2\n",
        "    predictions = torch.sigmoid(output_layer)\n",
        "\n",
        "    # Compute the loss function (Mean Squared Error)\n",
        "    loss = torch.mean((predictions - y_train) ** 2)\n",
        "\n",
        "    # Backward propagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Update weights using gradient descent\n",
        "    with torch.no_grad():\n",
        "        w1 -= learning_rate * w1.grad\n",
        "        b1 -= learning_rate * b1.grad\n",
        "        w2 -= learning_rate * w2.grad\n",
        "        b2 -= learning_rate * b2.grad\n",
        "\n",
        "        # Clear gradients\n",
        "        w1.grad.zero_()\n",
        "        b1.grad.zero_()\n",
        "        w2.grad.zero_()\n",
        "        b2.grad.zero_()\n",
        "\n",
        "    # Print the loss every 100 epochs\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Test the model\n",
        "test_input = torch.tensor([[0.7, 0.8], [0.1, 0.1]], dtype=torch.float32).to(device)\n",
        "with torch.no_grad():\n",
        "    hidden_layer = torch.matmul(test_input, w1) + b1\n",
        "    hidden_layer_activation = torch.sigmoid(hidden_layer)\n",
        "    output_layer = torch.matmul(hidden_layer_activation, w2) + b2\n",
        "    predictions = torch.sigmoid(output_layer)\n",
        "    print('Test Predictions:', predictions.cpu().numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEeGp0M-oHCy",
        "outputId": "bd2ed974-e9ff-4cd6-badb-3c1b07af48bf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/1000], Loss: 0.2648\n",
            "Epoch [200/1000], Loss: 0.2590\n",
            "Epoch [300/1000], Loss: 0.2563\n",
            "Epoch [400/1000], Loss: 0.2549\n",
            "Epoch [500/1000], Loss: 0.2540\n",
            "Epoch [600/1000], Loss: 0.2533\n",
            "Epoch [700/1000], Loss: 0.2528\n",
            "Epoch [800/1000], Loss: 0.2522\n",
            "Epoch [900/1000], Loss: 0.2517\n",
            "Epoch [1000/1000], Loss: 0.2512\n",
            "Test Predictions: [[0.4247769 ]\n",
            " [0.45901698]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "wG7IwHP7hxnC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9hXkkK3gnBD",
        "outputId": "b4251cea-4065-4a62-91da-fdd19ff8091a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class LogisticRegression:\n",
        "    def __init__(self, lr=0.01, n_iter=1000):\n",
        "        self.lr = lr\n",
        "        self.n_iter = n_iter\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.n_iter):\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            y_pred = self.sigmoid(linear_model)\n",
        "\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
        "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
        "\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        y_pred = self.sigmoid(linear_model)\n",
        "        return [1 if i > 0.5 else 0 for i in y_pred]\n",
        "\n",
        "# Example usage:\n",
        "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])  # Features\n",
        "y = np.array([0, 0, 1, 1])  # Target\n",
        "\n",
        "log_reg = LogisticRegression(lr=0.1, n_iter=1000)\n",
        "log_reg.fit(X, y)\n",
        "print(log_reg.predict(X))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ridge Regression"
      ],
      "metadata": {
        "id": "HXSj4eIYh1A_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RidgeRegression:\n",
        "    def __init__(self, lr=0.01, n_iter=1000, alpha=0.1):\n",
        "        self.lr = lr\n",
        "        self.n_iter = n_iter\n",
        "        self.alpha = alpha\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.n_iter):\n",
        "            y_pred = np.dot(X, self.weights) + self.bias\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y)) + (self.alpha / n_samples) * self.weights\n",
        "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
        "\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.dot(X, self.weights) + self.bias\n",
        "\n",
        "# Example usage:\n",
        "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])  # Features\n",
        "y = np.array([5, 7, 9, 11])  # Target\n",
        "\n",
        "ridge_reg = RidgeRegression(lr=0.01, n_iter=1000, alpha=0.1)\n",
        "ridge_reg.fit(X, y)\n",
        "print(ridge_reg.predict(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlyhMIkRh5bB",
        "outputId": "d3c862a6-65d8-42db-c795-3869550f40e6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 4.85180138  6.92572064  8.9996399  11.07355916]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support Vector Machine, SVM"
      ],
      "metadata": {
        "id": "XhF_zlIRh-cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SVM:\n",
        "    def __init__(self, lr=0.001, n_iter=1000, lambda_param=0.01):\n",
        "        self.lr = lr\n",
        "        self.n_iter = n_iter\n",
        "        self.lambda_param = lambda_param\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y_ = np.where(y <= 0, -1, 1)\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.n_iter):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                condition = y_[idx] * (np.dot(x_i, self.weights) - self.bias) >= 1\n",
        "                if condition:\n",
        "                    self.weights -= self.lr * (2 * self.lambda_param * self.weights)\n",
        "                else:\n",
        "                    self.weights -= self.lr * (2 * self.lambda_param * self.weights - np.dot(x_i, y_[idx]))\n",
        "                    self.bias -= self.lr * y_[idx]\n",
        "\n",
        "    def predict(self, X):\n",
        "        approx = np.dot(X, self.weights) - self.bias\n",
        "        return np.sign(approx)\n",
        "\n",
        "# Example usage:\n",
        "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])  # Features\n",
        "y = np.array([0, 1, 0, 1])  # Target\n",
        "\n",
        "svm = SVM()\n",
        "svm.fit(X, y)\n",
        "print(svm.predict(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c28SAQ5AiHnh",
        "outputId": "722d001c-0a32-413d-a227-a5a166d19a40"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.  1.  1.  1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Nearest Neighbors, KNN"
      ],
      "metadata": {
        "id": "CdHe9ZBmiL1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = [self._predict(x) for x in X]\n",
        "        return np.array(y_pred)\n",
        "\n",
        "    def _predict(self, x):\n",
        "        distances = [np.sqrt(np.sum((x - x_train) ** 2)) for x_train in self.X_train]\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "        most_common = np.bincount(k_nearest_labels).argmax()\n",
        "        return most_common\n",
        "\n",
        "# Example usage:\n",
        "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])  # Features\n",
        "y = np.array([0, 0, 1, 1])  # Target\n",
        "\n",
        "knn = KNN(k=3)\n",
        "knn.fit(X, y)\n",
        "print(knn.predict(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OTFfJ8-iPIr",
        "outputId": "86e28eb5-f705-47f1-8b18-395fc54d5478"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree"
      ],
      "metadata": {
        "id": "Gf7EGc6PiSVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=10):\n",
        "        self.max_depth = max_depth\n",
        "        self.tree = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.tree = self._grow_tree(X, y)\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        n_samples, n_features = X.shape\n",
        "        if n_samples == 0 or depth >= self.max_depth:\n",
        "            return np.bincount(y).argmax()\n",
        "\n",
        "        best_split = self._best_split(X, y)\n",
        "        if best_split[\"gain\"] == 0:\n",
        "            return np.bincount(y).argmax()\n",
        "\n",
        "        left_idx, right_idx = best_split[\"groups\"]\n",
        "        left = self._grow_tree(X[left_idx], y[left_idx], depth + 1)\n",
        "        right = self._grow_tree(X[right_idx], y[right_idx], depth + 1)\n",
        "        return {\"feature\": best_split[\"feature\"], \"threshold\": best_split[\"threshold\"], \"left\": left, \"right\": right}\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        best_gain = -1\n",
        "        split = None\n",
        "        for feature_idx in range(X.shape[1]):\n",
        "            for threshold in np.unique(X[:, feature_idx]):\n",
        "                groups = self._split(X[:, feature_idx], threshold)\n",
        "                gain = self._information_gain(y, groups)\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    split = {\"feature\": feature_idx, \"threshold\": threshold, \"gain\": gain, \"groups\": groups}\n",
        "        return split\n",
        "\n",
        "    def _split(self, X_column, threshold):\n",
        "        left_idx = np.where(X_column < threshold)[0]\n",
        "        right_idx = np.where(X_column >= threshold)[0]\n",
        "        return left_idx, right_idx\n",
        "\n",
        "    def _information_gain(self, y, groups):\n",
        "        n = len(y)\n",
        "        p_left, p_right = len(groups[0]) / n, len(groups[1]) / n\n",
        "        gain = self._entropy(y) - (p_left * self._entropy(y[groups[0]]) + p_right * self._entropy(y[groups[1]]))\n",
        "        return gain\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        proportions = np.bincount(y) / len(y)\n",
        "        return -np.sum([p * np.log2(p) for p in proportions if p > 0])\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        if isinstance(node, dict):\n",
        "            if x[node[\"feature\"]] < node[\"threshold\"]:\n",
        "                return self._traverse_tree(x, node[\"left\"])\n",
        "            else:\n",
        "                return self._traverse_tree(x, node[\"right\"])\n",
        "        else:\n",
        "            return node\n",
        "\n",
        "# Example usage:\n",
        "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])  # Features\n",
        "y = np.array([0, 0, 1, 1])  # Target\n",
        "\n",
        "tree = DecisionTree(max_depth=3)\n",
        "tree.fit(X, y)\n",
        "print(tree.predict(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GN9YYpLiXTP",
        "outputId": "2057600e-9df8-4e60-bf3a-5ed325d1826a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "uXvsAVfeieJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomForest:\n",
        "    def __init__(self, n_trees=10, max_depth=10):\n",
        "        self.n_trees = n_trees\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for _ in range(self.n_trees):\n",
        "            indices = np.random.choice(len(y), size=len(y), replace=True)\n",
        "            X_sample, y_sample = X[indices], y[indices]\n",
        "            tree = DecisionTree(max_depth=self.max_depth)\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "        tree_preds = np.swapaxes(tree_preds, 0, 1)\n",
        "        return [np.bincount(tree_pred).argmax() for tree_pred in tree_preds]\n",
        "\n",
        "# Example usage:\n",
        "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])  # Features\n",
        "y = np.array([0, 0, 1, 1])  # Target\n",
        "\n",
        "forest = RandomForest(n_trees=5, max_depth=3)\n",
        "forest.fit(X, y)\n",
        "print(forest.predict(X))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euOLOM0Xij53",
        "outputId": "7f1d2c78-8e4e-4f9d-85a5-fa29871e84b0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost"
      ],
      "metadata": {
        "id": "iVdCg1X9in-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class XGBoost:\n",
        "    def __init__(self, n_estimators=100, lr=0.1, max_depth=3, lambda_reg=1.0):\n",
        "        self.n_estimators = n_estimators      # Number of trees\n",
        "        self.lr = lr                          # Learning rate\n",
        "        self.max_depth = max_depth            # Maximum depth of each tree\n",
        "        self.lambda_reg = lambda_reg          # Regularization parameter\n",
        "        self.trees = []                       # Store each tree\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Ensure y and residual are float types\n",
        "        y = y.astype(np.float64)\n",
        "        residual = np.copy(y)  # Initial residual\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            # Use first and second derivatives (Gradient and Hessian) to fit decision trees\n",
        "            gradient = self._gradient(y, residual)\n",
        "            hessian = self._hessian(y, residual)\n",
        "\n",
        "            tree = self._build_tree(X, gradient, hessian, depth=0)\n",
        "            y_pred = self._predict_tree(tree, X)\n",
        "\n",
        "            # Update residual\n",
        "            residual -= self.lr * y_pred\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Initial predictions are zero (assume all initial predictions are 0)\n",
        "        pred = np.zeros(X.shape[0])\n",
        "        for tree in self.trees:\n",
        "            pred += self.lr * self._predict_tree(tree, X)\n",
        "        return pred\n",
        "\n",
        "    def _gradient(self, y, y_pred):\n",
        "        # Gradient is the first derivative of the loss function w.r.t predictions\n",
        "        return y_pred - y\n",
        "\n",
        "    def _hessian(self, y, y_pred):\n",
        "        # Hessian (second derivative of the loss function)\n",
        "        return np.ones_like(y_pred)\n",
        "\n",
        "    def _build_tree(self, X, gradient, hessian, depth):\n",
        "        if depth >= self.max_depth or len(X) <= 1:\n",
        "            leaf_value = -np.sum(gradient) / (np.sum(hessian) + self.lambda_reg)\n",
        "            return {\"leaf\": leaf_value}\n",
        "\n",
        "        # Iterate over features and all possible split points to find the best split\n",
        "        n_samples, n_features = X.shape\n",
        "        best_gain = -float('inf')\n",
        "        split = None\n",
        "        for feature_idx in range(n_features):\n",
        "            thresholds = np.unique(X[:, feature_idx])\n",
        "            for threshold in thresholds:\n",
        "                left_idx, right_idx = self._split(X[:, feature_idx], threshold)\n",
        "                if len(left_idx) == 0 or len(right_idx) == 0:\n",
        "                    continue\n",
        "\n",
        "                gain = self._calc_gain(gradient, hessian, left_idx, right_idx)\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    split = {\"feature_idx\": feature_idx, \"threshold\": threshold,\n",
        "                             \"left_idx\": left_idx, \"right_idx\": right_idx}\n",
        "\n",
        "        if best_gain == -float('inf'):\n",
        "            leaf_value = -np.sum(gradient) / (np.sum(hessian) + self.lambda_reg)\n",
        "            return {\"leaf\": leaf_value}\n",
        "\n",
        "        # Build left and right subtrees\n",
        "        left_subtree = self._build_tree(X[split[\"left_idx\"]], gradient[split[\"left_idx\"]],\n",
        "                                        hessian[split[\"left_idx\"]], depth + 1)\n",
        "        right_subtree = self._build_tree(X[split[\"right_idx\"]], gradient[split[\"right_idx\"]],\n",
        "                                         hessian[split[\"right_idx\"]], depth + 1)\n",
        "\n",
        "        return {\"feature_idx\": split[\"feature_idx\"], \"threshold\": split[\"threshold\"],\n",
        "                \"left\": left_subtree, \"right\": right_subtree}\n",
        "\n",
        "    def _calc_gain(self, gradient, hessian, left_idx, right_idx):\n",
        "        # Calculate the gain for the split: gain = G_left^2 / H_left + G_right^2 / H_right - (G_total^2 / H_total)\n",
        "        G_left = np.sum(gradient[left_idx])\n",
        "        H_left = np.sum(hessian[left_idx])\n",
        "        G_right = np.sum(gradient[right_idx])\n",
        "        H_right = np.sum(hessian[right_idx])\n",
        "\n",
        "        G_total = G_left + G_right\n",
        "        H_total = H_left + H_right\n",
        "\n",
        "        gain = 0.5 * (G_left**2 / (H_left + self.lambda_reg) + G_right**2 / (H_right + self.lambda_reg)\n",
        "                      - G_total**2 / (H_total + self.lambda_reg))\n",
        "        return gain\n",
        "\n",
        "    def _split(self, X_column, threshold):\n",
        "        # Split data based on a feature column and threshold\n",
        "        left_idx = np.where(X_column < threshold)[0]\n",
        "        right_idx = np.where(X_column >= threshold)[0]\n",
        "        return left_idx, right_idx\n",
        "\n",
        "    def _predict_tree(self, tree, X):\n",
        "        # Predict values for given samples X\n",
        "        if \"leaf\" in tree:\n",
        "            return np.ones(X.shape[0]) * tree[\"leaf\"]\n",
        "\n",
        "        feature_idx = tree[\"feature_idx\"]\n",
        "        threshold = tree[\"threshold\"]\n",
        "\n",
        "        left_idx = np.where(X[:, feature_idx] < threshold)[0]\n",
        "        right_idx = np.where(X[:, feature_idx] >= threshold)[0]\n",
        "\n",
        "        pred = np.zeros(X.shape[0])\n",
        "        pred[left_idx] = self._predict_tree(tree[\"left\"], X[left_idx])\n",
        "        pred[right_idx] = self._predict_tree(tree[\"right\"], X[right_idx])\n",
        "\n",
        "        return pred\n",
        "\n",
        "# Example usage\n",
        "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])  # Features\n",
        "y = np.array([5, 7, 9, 11, 13])  # Target\n",
        "\n",
        "xgb = XGBoost(n_estimators=5, lr=0.1, max_depth=3, lambda_reg=1.0)\n",
        "xgb.fit(X, y)\n",
        "predictions = xgb.predict(X)\n"
      ],
      "metadata": {
        "id": "_Iz1F_AnipFP"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}